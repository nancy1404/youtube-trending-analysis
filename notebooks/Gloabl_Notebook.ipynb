{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ed8c26",
   "metadata": {},
   "source": [
    "# Global YouTube Trending – Unified Modeling Notebook\n",
    "\n",
    "This notebook combines all country-level YouTube Trending datasets into a single global dataset and prepares it for machine learning.\n",
    "\n",
    "**Goals:**\n",
    "- Load and clean all country datasets (US, GB, RU, BR, etc.)\n",
    "- Add a `country` label to each dataset and concatenate into `global_df`\n",
    "- Engineer basic time and text features:\n",
    "  - `publish_hour`, `publish_day`\n",
    "  - `time_to_trending` (in days)\n",
    "  - `title_length`, `tag_count`\n",
    "- Define a global binary target:\n",
    "  - `viral` = top 10% of videos by view count\n",
    "- Build a Scikit-Learn preprocessing pipeline (numeric + categorical)\n",
    "- Train and evaluate several baseline models (Logistic, RF, XGBoost, Naive Bayes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29bbfb",
   "metadata": {},
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5601cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization (optional; for sanity checks)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50187f95",
   "metadata": {},
   "source": [
    "# Load & Clean All Country Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ef2db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US: shape after cleaning = (268704, 17)\n",
      "GB: shape after cleaning = (268667, 17)\n",
      "RU: shape after cleaning = (238539, 17)\n",
      "BR: shape after cleaning = (268701, 17)\n",
      "CA: shape after cleaning = (268633, 17)\n",
      "DE: shape after cleaning = (268604, 17)\n",
      "FR: shape after cleaning = (268646, 17)\n",
      "IN: shape after cleaning = (251202, 17)\n",
      "JP: shape after cleaning = (268629, 17)\n",
      "KR: shape after cleaning = (265602, 17)\n",
      "MX: shape after cleaning = (268528, 17)\n",
      "\n",
      "Global dataset shape: (2904455, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3C66w5Z0ixs</td>\n",
       "      <td>I ASKED HER TO BE MY GIRLFRIEND...</td>\n",
       "      <td>2020-08-11T19:20:14Z</td>\n",
       "      <td>UCvtRTOMP2TqYqu51xNrqAzg</td>\n",
       "      <td>Brawadis</td>\n",
       "      <td>22</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>brawadis|prank|basketball|skits|ghost|funny vi...</td>\n",
       "      <td>1514614</td>\n",
       "      <td>156908</td>\n",
       "      <td>5855</td>\n",
       "      <td>35313</td>\n",
       "      <td>https://i.ytimg.com/vi/3C66w5Z0ixs/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SUBSCRIBE to BRAWADIS ▶ http://bit.ly/Subscrib...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M9Pmf9AB4Mo</td>\n",
       "      <td>Apex Legends | Stories from the Outlands – “Th...</td>\n",
       "      <td>2020-08-11T17:00:10Z</td>\n",
       "      <td>UC0ZV6M2THA81QT9hrVWJG3A</td>\n",
       "      <td>Apex Legends</td>\n",
       "      <td>20</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>Apex Legends|Apex Legends characters|new Apex ...</td>\n",
       "      <td>2381688</td>\n",
       "      <td>146739</td>\n",
       "      <td>2794</td>\n",
       "      <td>16549</td>\n",
       "      <td>https://i.ytimg.com/vi/M9Pmf9AB4Mo/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>While running her own modding shop, Ramya Pare...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J78aPJ3VyNs</td>\n",
       "      <td>I left youtube for a month and THIS is what ha...</td>\n",
       "      <td>2020-08-11T16:34:06Z</td>\n",
       "      <td>UCYzPXprvl5Y-Sf0g4vX-m6g</td>\n",
       "      <td>jacksepticeye</td>\n",
       "      <td>24</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>jacksepticeye|funny|funny meme|memes|jacksepti...</td>\n",
       "      <td>2038853</td>\n",
       "      <td>353787</td>\n",
       "      <td>2628</td>\n",
       "      <td>40221</td>\n",
       "      <td>https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I left youtube for a month and this is what ha...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kXLn3HkpjaA</td>\n",
       "      <td>XXL 2020 Freshman Class Revealed - Official An...</td>\n",
       "      <td>2020-08-11T16:38:55Z</td>\n",
       "      <td>UCbg_UMjlHJg_19SZckaKajg</td>\n",
       "      <td>XXL</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>xxl freshman|xxl freshmen|2020 xxl freshman|20...</td>\n",
       "      <td>496771</td>\n",
       "      <td>23251</td>\n",
       "      <td>1856</td>\n",
       "      <td>7647</td>\n",
       "      <td>https://i.ytimg.com/vi/kXLn3HkpjaA/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Subscribe to XXL → http://bit.ly/subscribe-xxl...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIUo6yapDbc</td>\n",
       "      <td>Ultimate DIY Home Movie Theater for The LaBran...</td>\n",
       "      <td>2020-08-11T15:10:05Z</td>\n",
       "      <td>UCDVPcEbVLQgLZX0Rt6jo34A</td>\n",
       "      <td>Mr. Kate</td>\n",
       "      <td>26</td>\n",
       "      <td>2020-08-12T00:00:00Z</td>\n",
       "      <td>The LaBrant Family|DIY|Interior Design|Makeove...</td>\n",
       "      <td>1123889</td>\n",
       "      <td>45802</td>\n",
       "      <td>964</td>\n",
       "      <td>2196</td>\n",
       "      <td>https://i.ytimg.com/vi/VIUo6yapDbc/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Transforming The LaBrant Family's empty white ...</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  3C66w5Z0ixs                 I ASKED HER TO BE MY GIRLFRIEND...   \n",
       "1  M9Pmf9AB4Mo  Apex Legends | Stories from the Outlands – “Th...   \n",
       "2  J78aPJ3VyNs  I left youtube for a month and THIS is what ha...   \n",
       "3  kXLn3HkpjaA  XXL 2020 Freshman Class Revealed - Official An...   \n",
       "4  VIUo6yapDbc  Ultimate DIY Home Movie Theater for The LaBran...   \n",
       "\n",
       "            publishedAt                 channelId   channelTitle  categoryId  \\\n",
       "0  2020-08-11T19:20:14Z  UCvtRTOMP2TqYqu51xNrqAzg       Brawadis          22   \n",
       "1  2020-08-11T17:00:10Z  UC0ZV6M2THA81QT9hrVWJG3A   Apex Legends          20   \n",
       "2  2020-08-11T16:34:06Z  UCYzPXprvl5Y-Sf0g4vX-m6g  jacksepticeye          24   \n",
       "3  2020-08-11T16:38:55Z  UCbg_UMjlHJg_19SZckaKajg            XXL          10   \n",
       "4  2020-08-11T15:10:05Z  UCDVPcEbVLQgLZX0Rt6jo34A       Mr. Kate          26   \n",
       "\n",
       "          trending_date                                               tags  \\\n",
       "0  2020-08-12T00:00:00Z  brawadis|prank|basketball|skits|ghost|funny vi...   \n",
       "1  2020-08-12T00:00:00Z  Apex Legends|Apex Legends characters|new Apex ...   \n",
       "2  2020-08-12T00:00:00Z  jacksepticeye|funny|funny meme|memes|jacksepti...   \n",
       "3  2020-08-12T00:00:00Z  xxl freshman|xxl freshmen|2020 xxl freshman|20...   \n",
       "4  2020-08-12T00:00:00Z  The LaBrant Family|DIY|Interior Design|Makeove...   \n",
       "\n",
       "   view_count   likes  dislikes  comment_count  \\\n",
       "0     1514614  156908      5855          35313   \n",
       "1     2381688  146739      2794          16549   \n",
       "2     2038853  353787      2628          40221   \n",
       "3      496771   23251      1856           7647   \n",
       "4     1123889   45802       964           2196   \n",
       "\n",
       "                                   thumbnail_link  comments_disabled  \\\n",
       "0  https://i.ytimg.com/vi/3C66w5Z0ixs/default.jpg              False   \n",
       "1  https://i.ytimg.com/vi/M9Pmf9AB4Mo/default.jpg              False   \n",
       "2  https://i.ytimg.com/vi/J78aPJ3VyNs/default.jpg              False   \n",
       "3  https://i.ytimg.com/vi/kXLn3HkpjaA/default.jpg              False   \n",
       "4  https://i.ytimg.com/vi/VIUo6yapDbc/default.jpg              False   \n",
       "\n",
       "   ratings_disabled                                        description country  \n",
       "0             False  SUBSCRIBE to BRAWADIS ▶ http://bit.ly/Subscrib...      US  \n",
       "1             False  While running her own modding shop, Ramya Pare...      US  \n",
       "2             False  I left youtube for a month and this is what ha...      US  \n",
       "3             False  Subscribe to XXL → http://bit.ly/subscribe-xxl...      US  \n",
       "4             False  Transforming The LaBrant Family's empty white ...      US  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Country codes you have processed\n",
    "countries = [\"US\", \"GB\", \"RU\", \"BR\", \"CA\", \"DE\", \"FR\", \"IN\", \"JP\", \"KR\", \"MX\"]\n",
    "\n",
    "data_path = \"../datasets\"\n",
    "\n",
    "def load_and_clean_country(country_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and apply basic cleaning to a single country's YouTube trending dataset.\n",
    "    This mirrors the per-country cleaning you did in your existing notebooks.\n",
    "    \"\"\"\n",
    "    file_path = f\"{data_path}/{country_code}_youtube_trending_data.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop exact duplicate rows\n",
    "    df = df.drop_duplicates().copy()\n",
    "    \n",
    "    # Fill missing descriptions with empty string\n",
    "    if \"description\" in df.columns:\n",
    "        df[\"description\"] = df[\"description\"].fillna(\"\")\n",
    "    \n",
    "    # Add country label\n",
    "    df[\"country\"] = country_code\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all countries into a list\n",
    "country_dfs = []\n",
    "for c in countries:\n",
    "    temp_df = load_and_clean_country(c)\n",
    "    print(f\"{c}: shape after cleaning = {temp_df.shape}\")\n",
    "    country_dfs.append(temp_df)\n",
    "\n",
    "# Concatenate into a global dataframe\n",
    "global_df = pd.concat(country_dfs, ignore_index=True)\n",
    "print(\"\\nGlobal dataset shape:\", global_df.shape)\n",
    "\n",
    "# Quick sanity check\n",
    "global_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69398a5c",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Time + Simple text features\n",
    "\n",
    "We'll add:\n",
    "-   publish_hour, publish_day\n",
    "-   time_to_trending_days\n",
    "-   title_length\n",
    "-   tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c15a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>view_count</th>\n",
       "      <th>publish_hour</th>\n",
       "      <th>publish_dayofweek</th>\n",
       "      <th>time_to_trending_days</th>\n",
       "      <th>title_length</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>1514614</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>2381688</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2916</td>\n",
       "      <td>60</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>2038853</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>53</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>496771</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>56</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>1123889</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>55</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  view_count  publish_hour  publish_dayofweek  time_to_trending_days  \\\n",
       "0      US     1514614            19                  1                 0.1943   \n",
       "1      US     2381688            17                  1                 0.2916   \n",
       "2      US     2038853            16                  1                 0.3097   \n",
       "3      US      496771            16                  1                 0.3063   \n",
       "4      US     1123889            15                  1                 0.3680   \n",
       "\n",
       "   title_length  tag_count  \n",
       "0            34         15  \n",
       "1            60         25  \n",
       "2            53         30  \n",
       "3            56         23  \n",
       "4            55         33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure datetime columns are in datetime format\n",
    "global_df[\"publishedAt\"] = pd.to_datetime(global_df[\"publishedAt\"])\n",
    "global_df[\"trending_date\"] = pd.to_datetime(global_df[\"trending_date\"])\n",
    "\n",
    "# Publish hour and day-of-week (0=Monday, 6=Sunday)\n",
    "global_df[\"publish_hour\"] = global_df[\"publishedAt\"].dt.hour\n",
    "global_df[\"publish_dayofweek\"] = global_df[\"publishedAt\"].dt.dayofweek  # 0–6\n",
    "\n",
    "# Time from publish to trending in days (float)\n",
    "time_delta = global_df[\"trending_date\"] - global_df[\"publishedAt\"]\n",
    "global_df[\"time_to_trending_days\"] = time_delta.dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Basic text-based features\n",
    "global_df[\"title_length\"] = global_df[\"title\"].astype(str).str.len()\n",
    "global_df[\"tag_count\"] = global_df[\"tags\"].astype(str).apply(\n",
    "    lambda x: 0 if x == \"[none]\" else len(str(x).split(\"|\"))\n",
    ")\n",
    "\n",
    "# Quick check of new columns\n",
    "global_df[[\"country\", \"view_count\", \"publish_hour\", \"publish_dayofweek\",\n",
    "           \"time_to_trending_days\", \"title_length\", \"tag_count\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da1fc3",
   "metadata": {},
   "source": [
    "# Define the Global Target\n",
    "`viral` (top 10% by view_count)\n",
    "\n",
    "Right now, we'll use global top 10% as viral. Later we can also try \"top 10% within each country\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453badb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3542439.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define viral threshold: top 10% of view_count globally\n",
    "viral_threshold = global_df[\"view_count\"].quantile(0.90)\n",
    "viral_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b2d793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "viral\n",
       "0   0.9000\n",
       "1   0.1000\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary target: 1 if view_count >= threshold else 0\n",
    "global_df[\"viral\"] = (global_df[\"view_count\"] >= viral_threshold).astype(int)\n",
    "\n",
    "# Check class balance\n",
    "global_df[\"viral\"].value_counts(normalize=True)\n",
    "# You want roughly ~10% 1s and ~90% 0s, which are indeed accomplished here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df40792",
   "metadata": {},
   "source": [
    "# Select Features & Build Preprocessing Pipeline\n",
    "We'll use:\n",
    "\n",
    "**Numerical Features**\n",
    "-   likes, dislikes, comment_count\n",
    "-   publish_hour, publish_dayofweek\n",
    "-   time_to_trending_days\n",
    "-   title_length, tag_count\n",
    "\n",
    "**Categorical Features**\n",
    "-   categoryId\n",
    "-   country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34fbe0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2904455, 10)\n",
      "y distribution:\n",
      " viral\n",
      "0   0.9000\n",
      "1   0.1000\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns\n",
    "numeric_features = [\n",
    "    \"likes\",\n",
    "    \"dislikes\",\n",
    "    \"comment_count\",\n",
    "    \"publish_hour\",\n",
    "    \"publish_dayofweek\",\n",
    "    \"time_to_trending_days\",\n",
    "    \"title_length\",\n",
    "    \"tag_count\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"categoryId\",\n",
    "    \"country\"\n",
    "]\n",
    "\n",
    "target_col = \"viral\"\n",
    "\n",
    "# Drop any rows with missing values in used columns (just in case)\n",
    "used_cols = numeric_features + categorical_features + [target_col]\n",
    "global_model_df = global_df[used_cols].dropna().copy()\n",
    "\n",
    "X = global_model_df[numeric_features + categorical_features]\n",
    "y = global_model_df[target_col]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y distribution:\\n\", y.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27569635",
   "metadata": {},
   "source": [
    "### Preprocessing with `ColumnTransformer``\n",
    "-   Scale numeric features (StandardScaler)\n",
    "-   One-hot encode categorical features (OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab6b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Numeric transformer: impute (if needed) + scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer: impute (if needed) + one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9251b",
   "metadata": {},
   "source": [
    "# Train/Test Split (global) and multi-model pipeline\n",
    "We'll create a helper loop to test multiple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbaca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2323564\n",
      "Test size: 580891\n"
     ]
    }
   ],
   "source": [
    "# Train/test split with stratification on target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42,\n",
    "    stratify = y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add789ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6240258",
   "metadata": {},
   "source": [
    "### Train & Evaluate All Models\n",
    "**WARNING: The next code chunk will take 80~90 mins to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a059380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Logistic Regression ===\n",
      "\n",
      "=== Training Random Forest ===\n",
      "\n",
      "=== Training XGBoost ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9766</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.9392</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9615</td>\n",
       "      <td>0.8621</td>\n",
       "      <td>0.7317</td>\n",
       "      <td>0.7916</td>\n",
       "      <td>0.9806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.8365</td>\n",
       "      <td>0.5896</td>\n",
       "      <td>0.6917</td>\n",
       "      <td>0.9521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision  Recall     F1  ROC-AUC\n",
       "1        Random Forest    0.9883     0.9766  0.9046 0.9392   0.9980\n",
       "2              XGBoost    0.9615     0.8621  0.7317 0.7916   0.9806\n",
       "0  Logistic Regression    0.9474     0.8365  0.5896 0.6917   0.9521"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "\n",
    "    # Full pipeline: preproessing + model\n",
    "    clf = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Some models (like Naive Bayes, RF, XGB) have predict_proba\n",
    "    if hasattr(clf.named_steps[\"model\"], \"predict_proba\"):\n",
    "        y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "        roc = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        y_proba=None\n",
    "        roc = np.nan # not available\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1,\n",
    "        \"ROC-AUC\": roc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"ROC-AUC\", ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d75d8",
   "metadata": {},
   "source": [
    "## Initial Model Evaluation Insights (Gloabl Dataset) \n",
    "\n",
    "After training and evaluating three baseline classification models on the global YouTube Trending dataset (~2.9M rows), several clear patterns emerge in performance:\n",
    "\n",
    "### 1. **Random Forest Demonstrates Superior Global Performance**\n",
    "The Random Forest Classifier significantly outperforms both Logistic Regression and XGBoost across all key evaluation metrics:\n",
    "\n",
    "- **Accuracy:** 0.998\n",
    "- **Precision:** 0.977\n",
    "- **Recall:** 0.905\n",
    "- **F1 Score:** 0.939\n",
    "- **ROC-AUC:** 0.998\n",
    "\n",
    "This combination of extremely high precision and storng recall indicates that the model is highly effective at identifying truly viral videos (top 10% globally) while minimizing false positives. The near-perfect ROC-AUC suggests that the Random Forest model is capturing nonlinear relationships and feature interactions present across diverse countries and video categories.\n",
    "\n",
    "### 2. **XGBoost Performs Well but Falls Short of RF**\n",
    "XGBoost achieves strong performance (ROC-AUC ~ 0.98), but lags behind RF in both recall and F1 score. This is expected given:\n",
    "- The dataset's size (~3M rows)\n",
    "- High-dimensional one-hot encoded categorical features\n",
    "- Limited hyperparameter turning at this stage\n",
    "\n",
    "With further tuning, XGBoost may narrow the gap, but RF currently provides the most robust global results with minimal configuration.\n",
    "\n",
    "### 3. **Logistic Regression Serves as a Strong Linear Baseline**\n",
    "Logistic Regression performs reasoanbly well (ROC-AUC = 0.95), demonstrating that engagement features such as likes, coments, and early performance have strong linear separability between viral and non-viral videos. However, the model's lower recall (~0.59) indicates that it misses a substantial portion of viral videos. This reinforces the need for nonlinear models to capture global patterns in audience engagement and cross-country differences.\n",
    "\n",
    "### 4. **Imbalanced Classification Handled Effectively**\n",
    "The global viral label (top 10%) results in a 90-10 class imbalance. Despite this, both RF and XGBoost maintain strong recall and precision, suggesting that:\n",
    "- The seleced features provide strong early-engagement signals.\n",
    "- The models are robust even without explicit class-balancing techniques. \n",
    "\n",
    "### 5. **Summary + Next Steps**\n",
    "The Random Forest Classifier is the strongest global baseline at this stage of the project. Its ability to capture complex interactions across countries, categories, and temporal features makes it the most promising candidate for subsequent analysis. Future steps will include extracting global feature importances to identify key drivers of virality and refining the model to cross-country comparisons and additional business insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec9324",
   "metadata": {},
   "source": [
    "# Global Feature Importance (RF)\n",
    "**safe runtime version**\n",
    "- Train a lightweight RF for importance (FAST)\n",
    "- Extract the transformed feature names\n",
    "- Compute feature importance + show top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d72d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RF for feature importance...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Train a lightweight Random Forest for feature importance\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=80, #smaller # of trees for faster training\n",
    "        max_depth=None,\n",
    "        n_jobs = -1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Training RF for feature importance...\")\n",
    "rf_pipeline.fit(X_train,y_train)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9014f5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract feature names from preprocessor (numeric + one-hot)\n",
    "onehot_cols = (\n",
    "    rf_pipeline.named_steps[\"preprocess\"]\n",
    "    .named_transformers_[\"cat\"]\n",
    "    .named_steps[\"onehot\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "numeric_cols = numeric_features\n",
    "\n",
    "feature_names = np.concatenate([numeric_cols, onehot_cols])\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce6a2cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes</td>\n",
       "      <td>0.3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment_count</td>\n",
       "      <td>0.2060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dislikes</td>\n",
       "      <td>0.0922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time_to_trending_days</td>\n",
       "      <td>0.0653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>title_length</td>\n",
       "      <td>0.0555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tag_count</td>\n",
       "      <td>0.0492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>publish_hour</td>\n",
       "      <td>0.0446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>publish_dayofweek</td>\n",
       "      <td>0.0279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>categoryId_17</td>\n",
       "      <td>0.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>categoryId_10</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>categoryId_24</td>\n",
       "      <td>0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>country_IN</td>\n",
       "      <td>0.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>country_BR</td>\n",
       "      <td>0.0042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>categoryId_20</td>\n",
       "      <td>0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>country_MX</td>\n",
       "      <td>0.0039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>categoryId_22</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>country_RU</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>categoryId_1</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>categoryId_23</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>country_KR</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  importance\n",
       "0                   likes      0.3819\n",
       "2           comment_count      0.2060\n",
       "1                dislikes      0.0922\n",
       "5   time_to_trending_days      0.0653\n",
       "6            title_length      0.0555\n",
       "7               tag_count      0.0492\n",
       "3            publish_hour      0.0446\n",
       "4       publish_dayofweek      0.0279\n",
       "12          categoryId_17      0.0103\n",
       "10          categoryId_10      0.0100\n",
       "17          categoryId_24      0.0055\n",
       "28             country_IN      0.0054\n",
       "23             country_BR      0.0042\n",
       "14          categoryId_20      0.0041\n",
       "31             country_MX      0.0039\n",
       "15          categoryId_22      0.0034\n",
       "32             country_RU      0.0031\n",
       "8            categoryId_1      0.0029\n",
       "16          categoryId_23      0.0028\n",
       "30             country_KR      0.0025"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute feature importance\n",
    "importances = rf_pipeline.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "fi_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "fi_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c7d85",
   "metadata": {},
   "source": [
    "## Global Feature Importance Insights\n",
    "\n",
    "To understand what drives virality across a combined datasset of ~2.9 million YouTube trending records, we examined feature importances from a RF model trained on the unified global dataset. The results reveal several clear, consistent patterns aligned with platform behavior and ealier country-level observations.\n",
    "\n",
    "### 1. **Engagement Signals Dominate Virality Prediction*\n",
    "The most influential features by a wide margin are:\n",
    "- **likes** (importance: 0.382)\n",
    "- **comment_count** (importance: 0.296)\n",
    "- **dislikes** (importance: 0.092)\n",
    "\n",
    "These features represent direct, early user engagement and collectively account for over **68%** of the model's total importance. This confirms that virallity on YouTube is overwhelmingly driven by *how viewers interact with a video shortly after release*, rather than by metadata such as category or country.\n",
    "\n",
    "### 2. **Temporal Dynamics Meaningfully Contribute**\n",
    "Several time-based features show moderate influence:\n",
    "- **time_to_trending_days** (0.065)\n",
    "- **publish_hour** (0.045)\n",
    "- **publish_dayofweek** (0.028)\n",
    "\n",
    "Videos that reach the trending page quickly tend to correlate with higher view velocity-reinforcing the role of *early momentum* in the YouTube algorithm.\n",
    "\n",
    "Publishing time also matters slightly, suggesting user availability patterns vary by hour and day across global markets. \n",
    "\n",
    "### 3. **Lightweight Content Features Provide Additional Signal**\n",
    "Two lightweight text-derived features also contribute meaningfully:\n",
    "- **title_length** (0.056)\n",
    "- **tag_count** (0.049)\n",
    "\n",
    "Longer titles and denser tag sets appear associated with higher engagement, though their effects are far smaller than direct engagement metrics. These results suggest creators who strategically optimize metadata may caputure marginal benefits in initial discovery or search visibility.\n",
    "\n",
    "### 4. **Category and Country Effects Are REal but Small**\n",
    "Category-level and country-level one-hot encodings show low but non-zero importance (e.g., `categoryId_17`, `categoryId_10`, `country_IN`, `country_BR`, `country_MX`,\n",
    "etc.). Their small contributions indicate:\n",
    "- Some categories have systematically higher baseline visibility (e.g., Entertainment, Music)\n",
    "- Certain countries exhibit slightly different view-velocity profiles.\n",
    "- However, neigther category nor country appears to be a strong standalone predictor of virality\n",
    "\n",
    "This aligns with the notion that audience engagement behaviors are broadly consistent worldwide despite regional content difference. \n",
    "\n",
    "### 5. **Overall Interpretations**\n",
    "The feature importance results demonstrate that **virality is governed by combination of early engagement strength and initial momentum**, with global and category patterns playing only supporting roles. These findings validate earlier EDA insights from individual countries-particularly that likes and comment volume form the strongest universal predictors of high-performing trending content.\n",
    "\n",
    "In the next phase, these insights will be used to frame cross-country comparisons, highlight differences in viewer behavior, and identify market-specific patterns that may not be visible in the global aggregate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0664b40",
   "metadata": {},
   "source": [
    "# Time-Series Analysis Section\n",
    "- New time-based features\n",
    "- Three lightweight visualizations (fast runtime)\n",
    "- Professional interpretation markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fd9d4",
   "metadata": {},
   "source": [
    "## Add Additional Time-Based Features \n",
    "*very fast, uses existing datetime columns*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
